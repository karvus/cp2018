\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[scaled=0.89]{inconsolata}
\author{Thomas Stenhaug \texttt{<tsten16@student.sdu.dk>}}
\usepackage[a4paper,top=2cm,bottom=2cm,left=4cm,right=4cm]{geometry}
\usepackage[iso,danish]{isodate}
\usepackage{fancyvrb}
%\usepackage{fancyhdr}
% \pagestyle{fancy}
% \fancyhf{}
% \lhead{T. Stenhaug}
% \rhead{DM519 exam \quad \quad \quad \today}
% \chead{}
% \lfoot{}
% \cfoot{\thepage}
% \rfoot{}
%\renewcommand{\footrulewidth}{0pt} %
%\setcounter{secnumdepth}{2}
%\setcounter{tocdepth}{2}
\date{\today}
\title{DM519 Spring 2018 Exam}
\begin{document}

\maketitle

\setlength{\baselineskip}{1.44\baselineskip}

\section{Methodology}

The central units of work in the project, are files containing a
plaintext line of comma separated integer-values.  I have chosen to
make a wrapper-class \texttt{NumberFile} around Java's \texttt{Path}s to
facilitate working with these files.

A trait shared between my solutions to \texttt{m1, m2} and
\texttt{m3}, is that they instantiate a
\texttt{BlockingDeque<NumberFile>} as the main shared datastructure for
production and consumption.  Initially, an executor for consumers,
with one consumer for each processor, is created.  A producer then
runs from the main thread, producing to the
\texttt{BlockingDeque<NumberFile>}.  The producer runs in the main
thread, because the operation is I/O-bound, so it didn't seem worth it
to invest complexity in distributing the work over threads.  (Although
it uses \texttt{parallel()} in a stream-chain, just in case the JRE
knows how to optimize it.)

While the main-thread is producing work-units, the consumers, one for
each processor, accepts and performs their work.

\subsection{Synchronizing when consumers are done}

For \texttt{m1} and \texttt{m3}, the mechanism for synchronizing the
consumers is a poison pill, that is fed to the queue of
\texttt{NumberFile}s.  The consumers gracefully exit their loop after
encountering the poison pill, so after finishing producing work, the
main thread shuts down the executor, and \texttt{await}s it, which
will block until consumers are all done.  In the case of \texttt{m1},
that concludes its job.  \texttt{m3} instead, needs to do some
additional computations on the collected data.  These computations are
run in parallel, governed by a separate executor.  Here, the
synchronization is implemented by waiting for said computations as
futures, while constructing the final result.

\texttt{m2} is different, in that instead of the result being an
aggregate of all files, it is an arbitrary element from the
\texttt{BlockingDeque<NumberFile>}, matching some criteria.  This
element is represented by a \texttt{CompletableFuture}, which is
waited for in the main thread after, production of
\texttt{NumberFiles} is completed, and is completed by the first
consumers that find an element matching the criteria.  Consumers exit
either when they discover that the \texttt{CompletableFuture} is
completed, or they are cancelled as a result of their executor being
shut down.

\subsection{Post processing}

While \texttt{m1} and \texttt{m2} are done with their work when their
consumers terminate, \texttt{m3} requires post-processing.  Its
consumers are also producers, adding work to a handful of concurrent
datastructures.   

\section{Advantages}

\subsection{Choice of datastructures, performance}

I chose to use ``off-the-shelf'' concurrent datastructures for this,
instead of writing my own.  In \texttt{m3}, for example, I keep a
\texttt{ConcurrentMap<Integer, LongAdder>} which keeps a tally of
number â†’ occurences.  Consumer threads are all writing to this
structure, so it is in high contention.  I update it like so
:

\begin{verbatim}
frequencies.computeIfAbsent(i, k -> new LongAdder()).increment();
\end{verbatim}

This results in lock-free, thread-safe operation on this
high-contention data.

One of the return values from \texttt{m3} is an ordered
\texttt{List<Path>}.  Initially I thought to populate a
\texttt{ConcurrentLinkedQueue} in the consumers, and obtain the
ordering during the post-processing.  This would be an \(O(n \lg n) \)
operation, in a single thread.  Instead I chose a
\texttt{ConcurrentSkipListSet}, which maintains $O(\lg n)$ for each
$n$ insertions, but with ``for free'' parallelization, since the
operation is spread across the already parallelized consumers.

TODO: add explanation about how I shortcut processing in m2,
specifically

\begin{verbatim}
Files.walk(dir)
    .filter(p -> Files.isRegularFile(p) && NumberFile.DAT_MATCHER.matches(p))
    // if we have already found a line-sum >= min, don't put more files in
   . allMatch(p-> paths.add(p) && !futureResult.isDone());
\end{verbatim}

\section{Limitations}

Every line in the files are read into memory in their entirety.  For
small inputs, this should be fine.  In a scenario where there are many
processors, little RAM and large lines, it could be a problem.

There are many occurrences of streams-usage, which would be likely be
faster if written out explicitly as loops.

Method-signatures are somewhat noisy and possibly hard to read,
especially for \texttt{StatsComputer}.  This could have been improved
by introducing abstractions over the arguments.

There is no rhyme nor rythm to exception-handling; instead, exceptions
are mostly silently ignored.

No tests are supplied, so changing the program requires
experimentation and eyeballing to assess its correctness.


\end{document}